{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "nb = nbformat.read(\"main.ipynb\", as_version=4)\n",
    "\n",
    "# Remove widgets metadata\n",
    "nb.metadata.pop(\"widgets\", None)\n",
    "\n",
    "nbformat.write(nb, \"main.ipynb\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MX4uuhEiY5JT"
   },
   "source": [
    "# EVA AI Hackathon — Submission Notebook Template\n",
    "\n",
    "**Challenge:** `CH-05 — < AI Social Listening (Off-page Focus)>`  \n",
    "**Participant / Team:** `<Omar Hamdy>`  \n",
    "**Date:** `2026-02-06`  \n",
    "**Runtime:** CPU / GPU (choose as needed)  \n",
    "**Offline compliance:** No external APIs (unless explicitly allowed by organizers)\n",
    "\n",
    "---\n",
    "\n",
    "## Submission Package (Required)\n",
    "This notebook is part of your final submission ZIP:\n",
    "\n",
    "```\n",
    "YourName_ProjectName.zip\n",
    "  main.ipynb\n",
    "  README.txt\n",
    "  technical_report.pdf\n",
    "  /data\n",
    "  /output\n",
    "```\n",
    "\n",
    "- `main.ipynb`: this notebook (end-to-end pipeline)\n",
    "- `README.txt` (max 1 page): run steps + expected runtime + hardware assumptions\n",
    "- `technical_report.pdf` (max 2–3 pages): approach + design choices + limitations + failure modes\n",
    "- `/output`: final outputs **with official file names and schemas**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PTKrPD2Y5JV"
   },
   "source": [
    "## Quick Start (How to use this notebook)\n",
    "1) Run cells from top to bottom.  \n",
    "2) Make sure your inputs are placed under `/data`.  \n",
    "3) Your code must write final submission files under `/output`.  \n",
    "4) Do **not** change output file names/columns unless the official schema requires it.\n",
    "\n",
    "> If the official dataset/schemas are released later, update only the **I/O Contract** section and the **Output Writer** section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bH4V0SqxY5JW",
    "outputId": "d4f5a3d2-5fc8-4a8c-d6da-e5f3fd1204c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.3.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
      "Pandas: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "# === 0) Setup: basic imports ===\n",
    "!pip install transformers torch pandas openpyxl scikit-learn tqdm\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Pandas:\", pd.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HLEhF6k7Y5JX",
    "outputId": "4e09603a-2813-494b-e60e-6904434b13af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Challenge: CH-05 - AI-Social-Listening-Off-Pages-Focus\n",
      "Team: <Omar Hamdy>\n",
      "SEED: 42\n",
      "DATA_DIR: /content/data/eva_social_listening_INPUT_FINAL 6(in) 1.xlsx\n",
      "OUTPUT_DIR: /content/output/social_listening_output (1).csv\n"
     ]
    }
   ],
   "source": [
    "# === 0.1) Configuration (EDIT THESE) ===\n",
    "CHALLENGE_CODE = \"CH-05\"\n",
    "CHALLENGE_TITLE = \"AI-Social-Listening-Off-Pages-Focus\"\n",
    "TEAM_NAME = \"<Omar Hamdy>\"\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "DATA_DIR = \"/content/data/eva_social_listening_INPUT_FINAL 6(in) 1.xlsx\"\n",
    "OUTPUT_DIR = \"/content/output/social_listening_output (1).csv\"\n",
    "\n",
    "print(\"Challenge:\", CHALLENGE_CODE, \"-\", CHALLENGE_TITLE)\n",
    "print(\"Team:\", TEAM_NAME)\n",
    "print(\"SEED:\", SEED)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aoz_mdCcY5JX"
   },
   "source": [
    "## 1) I/O Contract (Update when official schema is released)\n",
    "\n",
    "### Input location\n",
    "- `/data/`\n",
    "\n",
    "### Output location\n",
    "- `/output/`\n",
    "\n",
    "### Expected input files (examples — replace with official list)\n",
    "- `<file_1>.csv` — short description\n",
    "- `<file_2>.csv` — short description\n",
    "\n",
    "### Required output files (examples — replace with official list)\n",
    "- `<output_file>.csv` — short description\n",
    "\n",
    "### Schema Reference\n",
    "- Official schema source: **Data & Schema Addendum**\n",
    "- This notebook must follow the official schema strictly (file names, columns, types, keys, constraints).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AII2RvboY5JY",
    "outputId": "2495f8e3-1371-4ae7-c8c6-9c1187d3b241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in /data:\n"
     ]
    }
   ],
   "source": [
    "# === 2) Inspect input folder ===\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    raise FileNotFoundError(f\"DATA_DIR not found: {DATA_DIR}. Please create it and upload files under /data.\")\n",
    "\n",
    "print(\"Files in /data:\")\n",
    "for root, _, files in os.walk(DATA_DIR):\n",
    "    for f in files:\n",
    "        rel = os.path.relpath(os.path.join(root, f), DATA_DIR)\n",
    "        print(\" -\", rel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7aLl7EeY5JY"
   },
   "source": [
    "## 3) Data Loading (Replace with your challenge-specific loading)\n",
    "\n",
    "**Goal:** Load required input files from `/data` and create clean DataFrames/objects for downstream steps.\n",
    "\n",
    "✅ Checklist\n",
    "- All required input files exist  \n",
    "- Correct encoding / delimiter  \n",
    "- Dates parsed correctly (if applicable)  \n",
    "- Keys are unique where required  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dXw5sufY5JY",
    "outputId": "f2a6dec7-bbd1-4199-bb02-f6435a5500d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: load your input files into data_objects / DataFrames.\n",
      "      brand_name                            Brand social media link  \\\n",
      "0  Eva Cosmetics  https://noon.com/product/EVA-0031/review/menti...   \n",
      "1  Eva Cosmetics  https://jumia.com/product/EVA-0047/review/ment...   \n",
      "2  Eva Cosmetics                  https://facebook.com/posts/716113   \n",
      "\n",
      "           product_mentioned Duration From Duration To Market  platform  \\\n",
      "0  Eva Strengthening Shampoo    2025-08-08  2026-02-03  Egypt      Noon   \n",
      "1     إيفا Roll-On Whitening    2025-08-08  2026-02-03  Egypt     Jumia   \n",
      "2          Eva Glycerin Soap    2025-08-08  2026-02-03  Egypt  Facebook   \n",
      "\n",
      "  Source Type language                                text  \\\n",
      "0    Off-page       en    Average product, nothing special   \n",
      "1    Off-page       ar             سعره مناسب وجودته عالية   \n",
      "2    Off-page       en  PSA: Eva is having a sale! 50% off   \n",
      "\n",
      "                                   engagement product_id     mention_id  \\\n",
      "0                       {'helpful_votes': 14}   EVA-0031  mention_00666   \n",
      "1                       {'helpful_votes': 16}   EVA-0047  mention_00625   \n",
      "2  {'likes': 73, 'comments': 69, 'shares': 5}   EVA-0057  mention_00116   \n",
      "\n",
      "  source_name  \n",
      "0   user_3502  \n",
      "1   user_8439  \n",
      "2   user_6661  \n"
     ]
    }
   ],
   "source": [
    "# === 3) Data Loading (TEMPLATE) ===\n",
    "# TODO: Replace with official file names and loading logic.\n",
    "# Example:\n",
    "# sales = pd.read_csv(os.path.join(DATA_DIR, \"sales.csv\"))\n",
    "\n",
    "data_objects = {}\n",
    "print(\"TODO: load your input files into data_objects / DataFrames.\")\n",
    "\n",
    "input_file = os.path.join(DATA_DIR, 'social_listening_structure__1_.xlsx')\n",
    "df = pd.read_excel(DATA_DIR)\n",
    "print(df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcTxR3B_Y5JZ"
   },
   "source": [
    "## 4) Data Validation & Cleaning (Keep it minimal but strict)\n",
    "\n",
    "✅ Checklist (adapt to your challenge)\n",
    "- No unexpected nulls in required fields  \n",
    "- Handle duplicates / invalid rows  \n",
    "- Validate ranges and allowed values  \n",
    "- Document assumptions in README / report  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MsqSP3rrY5JZ",
    "outputId": "df17aa82-fd6e-4f27-c64b-96e1a4be1e8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: apply validation checks here.\n"
     ]
    }
   ],
   "source": [
    "# === 4) Validation & Cleaning (TEMPLATE) ===\n",
    "# TODO: Implement essential checks based on official schema.\n",
    "def require_columns(df: pd.DataFrame, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "print(\"TODO: apply validation checks here.\")\n",
    "\n",
    "def clean_text(text):\n",
    "    # Handle null/empty\n",
    "    if pd.isna(text) or text == '':\n",
    "        return \"No content\"\n",
    "\n",
    "    # Convert to string\n",
    "    text = str(text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Strip\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPekM-V0Y5JZ"
   },
   "source": [
    "## 5) Feature Engineering / Core Logic\n",
    "\n",
    "- If ML: feature extraction, encoding, and transformations  \n",
    "- If Optimization: define objective and constraints  \n",
    "- If NLP/CV: preprocessing + embeddings/feature pipeline  \n",
    "- Avoid leakage: do not use future information in training/validation  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395,
     "referenced_widgets": [
      "ee3d8635eab44b39a01d9c939eadf499",
      "4fb6ebfccf81413a9b02d10758da3d90",
      "8dbc1b9205b64648ac1b9218a2bf0b90",
      "d505b784ceca4e539b15aa43cef68d00",
      "b306e36ff37b4f53b3ab2baa731cdd92",
      "84135c488992458c9fa89ff02e169c54",
      "b405e5d4384741b0b70b7b99afcad364",
      "fe32afd5e2fe42abb4318b2a1616c898",
      "5af225c2dbbb4556ae41438e558e1a8a",
      "4f79ed49409447628e002e81b2b544c6",
      "e5ff52f02087476b8ad94d0df901effc",
      "b1aefb09f40e479d9f374fe557210130",
      "13dbfd78a15e4f3299bbe565b3beda09",
      "f344af7393a14722af2de1eb3e5df3c0",
      "47bf55ba691d4c618dc685b7a1dec9de",
      "e7baff0675394784a44e1c459fad02e1",
      "f4481e636f62401e9ce522c014a9af3e",
      "6e97b7deacdb485ebd37f68b007ba048",
      "44fc52bad90d4896b2fd6c32e1b54569",
      "02c8c9662e094205851158206cfd4474",
      "0ee0b0a170c9452d8631966f069301b0",
      "0fbb37ab4b914f30be82d0500df040bd"
     ]
    },
    "id": "FmjGhwpPY5Ja",
    "outputId": "3de686e2-07d8-473a-98ae-8e7341bafd16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: implement your feature engineering / optimization logic here.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3d8635eab44b39a01d9c939eadf499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification LOAD REPORT from: cardiffnlp/twitter-roberta-base-sentiment-latest\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.pooler.dense.bias       | UNEXPECTED |  | \n",
      "roberta.pooler.dense.weight     | UNEXPECTED |  | \n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1aefb09f40e479d9f374fe557210130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification LOAD REPORT from: CAMeL-Lab/bert-base-arabic-camelbert-msa-sentiment\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "# === 5) Feature Engineering / Core Logic (TEMPLATE) ===\n",
    "print(\"TODO: implement your feature engineering / optimization logic here.\")\n",
    "\n",
    "\n",
    "# English sentiment model\n",
    "sentiment_en = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    ")\n",
    "\n",
    "# Arabic sentiment model\n",
    "sentiment_ar = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"CAMeL-Lab/bert-base-arabic-camelbert-msa-sentiment\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def classify_sentiment(text, language):\n",
    "    \"\"\"\n",
    "    Classify sentiment using language-specific models\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle empty text\n",
    "        if not text or len(text.strip()) < 3:\n",
    "            return 'neutral'\n",
    "\n",
    "        # Truncate long text (model limit)\n",
    "        text = text[:512]\n",
    "\n",
    "        # Use appropriate model based on language\n",
    "        if language == 'ar':\n",
    "            result = sentiment_ar(text)[0]\n",
    "            label = result['label'].lower()\n",
    "\n",
    "            if 'pos' in label:\n",
    "                return 'positive'\n",
    "            elif 'neg' in label:\n",
    "                return 'negative'\n",
    "            else:\n",
    "                return 'neutral'\n",
    "\n",
    "        else:  # English\n",
    "            result = sentiment_en(text)[0]\n",
    "            label = result['label'].lower()\n",
    "\n",
    "            if 'pos' in label:\n",
    "                return 'positive'\n",
    "            elif 'neg' in label:\n",
    "                return 'negative'\n",
    "            else:\n",
    "                return 'neutral'\n",
    "\n",
    "    except:\n",
    "        return 'neutral'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zY6ZJyPoY5Ja"
   },
   "source": [
    "## 6) Training / Inference\n",
    "\n",
    "- Baseline first, then improved approach  \n",
    "- Keep runtime reasonable  \n",
    "- Log key artifacts (scores, parameters, counts)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4_wFiCXqrNe_",
    "outputId": "fdc801f0-4cf5-417b-8a0d-2b10bda48c38"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:00<00:00, 200725.56it/s]\n",
      "100%|██████████| 1400/1400 [00:00<00:00, 282417.55it/s]\n",
      "100%|██████████| 1400/1400 [02:11<00:00, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentiment Distribution:\n",
      "sentiment\n",
      "positive    559\n",
      "neutral     425\n",
      "negative    416\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === 6) Training / Inference (TEMPLATE) ===\n",
    "\n",
    "(\"TODO: train model or run main inference pipeline here.\")\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df['cleaned_text'] = df['text'].progress_apply(clean_text)\n",
    "df['cleaned_product_mentioned'] = df['product_mentioned'].progress_apply(clean_text)\n",
    "\n",
    "\n",
    "# Apply sentiment classification\n",
    "df['sentiment'] = df.progress_apply(\n",
    "    lambda row: classify_sentiment(row['cleaned_text'], row['language']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nSentiment Distribution:\")\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MGjewSVkY5Ja",
    "outputId": "6071945c-77bb-4318-819f-8506afa84b46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400/1400 [00:00<00:00, 68518.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Topic Distribution:\n",
      "topic\n",
      "general_mention     675\n",
      "price_value         174\n",
      "smell_fragrance     118\n",
      "moisturizing         88\n",
      "skin_reaction        80\n",
      "hair_care            72\n",
      "customer_service     70\n",
      "product_quality      70\n",
      "longevity            53\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Average relevance: 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === 6) Training / Inference (TEMPLATE) ===\n",
    "\n",
    "# Define topic keywords\n",
    "topic_keywords = {\n",
    "    'product_quality': ['quality', 'effective', 'works', 'result', 'جودة', 'فعال', 'نتيجة', 'تأثير', 'مفيد'],\n",
    "    'smell_fragrance': ['smell', 'scent', 'fragrance', 'perfume', 'ريحة', 'رائحة', 'عطر'],\n",
    "    'price_value': ['price', 'expensive', 'cheap', 'cost', 'value', 'سعر', 'غالي', 'رخيص', 'تمن', 'فلوس'],\n",
    "    'moisturizing': ['moisturize', 'dry', 'hydrat', 'soft', 'smooth', 'ترطيب', 'نشف', 'ناعم', 'رطوبة'],\n",
    "    'longevity': ['last', 'lasting', 'stay', 'fade', 'ثبات', 'يدوم', 'يفضل', 'يروح'],\n",
    "    'skin_reaction': ['irritat', 'allerg', 'rash', 'sensitive', 'burn', 'حساسية', 'تهيج', 'احمرار', 'حرق'],\n",
    "    'hair_care': ['hair', 'shampoo', 'conditioner', 'شعر', 'شامبو', 'بلسم'],\n",
    "    'customer_service': ['service', 'deliver', 'shipping', 'support', 'توصيل', 'خدمة', 'شحن'],\n",
    "    'packaging': ['packag', 'bottle', 'container', 'box', 'عبوة', 'زجاجة', 'علبة']\n",
    "}\n",
    "\n",
    "def classify_topic(text):\n",
    "    \"\"\"Simple keyword-based topic classification\"\"\"\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Count keyword matches for each topic\n",
    "    topic_scores = {}\n",
    "    for topic, keywords in topic_keywords.items():\n",
    "        score = sum(1 for keyword in keywords if keyword in text_lower)\n",
    "        topic_scores[topic] = score\n",
    "\n",
    "    # Get topic with highest score\n",
    "    if max(topic_scores.values()) > 0:\n",
    "        return max(topic_scores, key=topic_scores.get)\n",
    "    else:\n",
    "        return 'general_mention'\n",
    "\n",
    "# Apply topic classification\n",
    "df['topic'] = df['cleaned_text'].progress_apply(classify_topic)\n",
    "\n",
    "print(\"\\n\\nTopic Distribution:\")\n",
    "print(df['topic'].value_counts())\n",
    "\n",
    "\n",
    "# Brand keywords\n",
    "brand_keywords = ['eva', 'ايفا', 'إيفا', 'evacosmetics', 'eva cosmetics']\n",
    "\n",
    "def calculate_relevance(text):\n",
    "    \"\"\"Simple relevance score based on brand mentions\"\"\"\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Check for brand keywords\n",
    "    for keyword in brand_keywords:\n",
    "        if keyword in text_lower:\n",
    "            return 1.0\n",
    "\n",
    "    return 0.7\n",
    "\n",
    "# Apply relevance scoring\n",
    "df['relevance_score'] = df['cleaned_text'].apply(calculate_relevance)\n",
    "\n",
    "print(f\"\\nAverage relevance: {df['relevance_score'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LXagKyjY5Ja"
   },
   "source": [
    "## 7) Generate Final Outputs (IMPORTANT)\n",
    "\n",
    "Write final outputs to `/output` with:\n",
    "- exact file names\n",
    "- exact columns and types\n",
    "- exact row-level requirements (unique keys, no duplicates if disallowed)\n",
    "- schema compliance gate (hard-fail if incorrect)\n",
    "\n",
    "✅ Schema Gate Checklist\n",
    "- Output columns match schema exactly  \n",
    "- Types match schema (int/float/string/date)  \n",
    "- Required fields not missing  \n",
    "- File name and folder correct  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "OT7HPEIJY5Ja",
    "outputId": "51ac9b84-8485-46f4-af3e-6252e7cae802"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder ready: /content/output/social_listening_output (1).csv\n",
      "TODO: write final output file(s) to /output.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_77282fff-2fa9-4e7a-912e-fc1c292eb099\", \"social_listening_output.csv\", 324665)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_fe355fcd-dec5-42f6-af6f-7c506fbc1ba4\", \"insights.json\", 1326)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === 7) Create /output and write files (TEMPLATE) ===\n",
    "#os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(\"Output folder ready:\", OUTPUT_DIR)\n",
    "\n",
    "# TODO: Replace with official output writer logic.\n",
    "# Example:\n",
    "# output_df.to_csv(os.path.join(OUTPUT_DIR, \"predictions.csv\"), index=False)\n",
    "\n",
    "print(\"TODO: write final output file(s) to /output.\")\n",
    "\n",
    "# Prepare output DataFrame\n",
    "output_df = df.copy()\n",
    "\n",
    "# Select required columns\n",
    "output_columns = [\n",
    "    'mention_id',\n",
    "    'platform',\n",
    "    'source_name',\n",
    "    'Brand social media link',\n",
    "    'text',\n",
    "    'brand_name',\n",
    "    'language',\n",
    "    'sentiment',\n",
    "    'topic',\n",
    "    'relevance_score',\n",
    "    'engagement',\n",
    "    'product_mentioned',\n",
    "    'product_id'\n",
    "]\n",
    "\n",
    "# Create final output\n",
    "final_output = output_df[output_columns].copy()\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "# Top Topics\n",
    "top_topics = final_output['topic'].value_counts().head(10)\n",
    "topics_list = [\n",
    "    {\n",
    "        'topic': topic,\n",
    "        'mention_count': int(count),\n",
    "        'percentage': round(count / len(final_output) * 100, 2)\n",
    "    }\n",
    "    for topic, count in top_topics.items()\n",
    "]\n",
    "\n",
    "# Sentiment Distribution\n",
    "sentiment_dist = final_output['sentiment'].value_counts()\n",
    "sentiment_breakdown = {\n",
    "    'positive': int(sentiment_dist.get('positive', 0)),\n",
    "    'negative': int(sentiment_dist.get('negative', 0)),\n",
    "    'neutral': int(sentiment_dist.get('neutral', 0))\n",
    "}\n",
    "\n",
    "# Alerts\n",
    "negative_ratio = sentiment_breakdown['negative'] / len(final_output)\n",
    "alerts = []\n",
    "\n",
    "if negative_ratio > 0.30:\n",
    "    alerts.append({\n",
    "        'type': 'high_negative_sentiment',\n",
    "        'severity': 'high',\n",
    "        'message': f'High negative sentiment: {negative_ratio:.1%}',\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "# Compile insights\n",
    "insights = {\n",
    "    'generation_timestamp': datetime.now().isoformat(),\n",
    "    'summary': {\n",
    "        'total_mentions': len(final_output),\n",
    "    },\n",
    "    'top_topics': topics_list,\n",
    "    'sentiment_distribution': sentiment_breakdown,\n",
    "    'platform_distribution': final_output['platform'].value_counts().to_dict(),\n",
    "    'language_distribution': final_output['language'].value_counts().to_dict(),\n",
    "    'alerts': alerts\n",
    "}\n",
    "\n",
    "# Save insights\n",
    "with open('insights.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(insights, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# CSV\n",
    "final_output.to_csv('social_listening_output.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download('social_listening_output.csv')\n",
    "files.download('insights.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNAdTV28Y5Ja",
    "outputId": "13caa1e9-e92c-47bc-b114-d7ecffb5f088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: validate output schema correctness here before submission.\n"
     ]
    }
   ],
   "source": [
    "# === 7.1) Minimal schema checks (TEMPLATE) ===\n",
    "# TODO: Replace with official schema checks.\n",
    "def assert_no_duplicates(df: pd.DataFrame, key_cols):\n",
    "    if df.duplicated(subset=key_cols).any():\n",
    "        raise ValueError(f\"Duplicates detected for key columns: {key_cols}\")\n",
    "assert_no_duplicates\n",
    "print(\"TODO: validate output schema correctness here before submission.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUbLJnKbY5Jb"
   },
   "source": [
    "## 8) Final Notes & Submission Checklist\n",
    "\n",
    "**Before you submit:**\n",
    "- `/output` contains the required file(s) with official schema  \n",
    "- Notebook runs end-to-end without manual edits  \n",
    "- `README.txt` includes run steps + runtime + hardware assumptions  \n",
    "- `technical_report.pdf` includes approach + design choices + limitations + failure modes  \n",
    "- Colab link is shared as: **Anyone with the link can view**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBRgLM8mkxIS"
   },
   "source": [
    "# **Accuracy Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RE3rIirMkwMR",
    "outputId": "c8d638b8-8cc3-45ff-d4c6-dd30e5e8e842"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Accuracy: 58.86%\n",
      "Correct: 824/1400\n"
     ]
    }
   ],
   "source": [
    "# Load ground truth\n",
    "ground_truth = pd.read_csv('/content/eva_ground_truth_FULL.csv')\n",
    "\n",
    "# Merge and compare\n",
    "comparison = final_output[['mention_id', 'sentiment']].merge(\n",
    "    ground_truth[['mention_id', 'sentiment']],\n",
    "    on='mention_id',\n",
    "    suffixes=('_predicted', '_actual')\n",
    ")\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = (comparison['sentiment_predicted'] == comparison['sentiment_actual']).sum()\n",
    "accuracy = correct / len(comparison) * 100\n",
    "\n",
    "print(f\"Sentiment Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Correct: {correct}/{len(comparison)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "nb = nbformat.read(\"main.ipynb\", as_version=4)\n",
    "\n",
    "# Remove widgets metadata\n",
    "nb.metadata.pop(\"widgets\", None)\n",
    "\n",
    "nbformat.write(nb, \"main.ipynb\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
